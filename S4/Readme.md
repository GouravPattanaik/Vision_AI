Group Members: Satya Nayak, Ramjee Ganti, Gourav Pattanaik, Jayant Ojha

- total number of parameters---14,112
- final validation/test accuracy---99.42%

Please refer the [notebook](https://github.com/GouravPattanaik/Vision_AI/tree/master/S4/EVA4_Session_4_Assignment.ipynb) 

## Logs:
epoch=1 loss=0.21764731407165527 batch_id=937: 100%|██████████| 938/938 [00:15<00:00, 59.40it/s]
Epoch 1--Train set: Average loss: 0.2176, Training Accuracy: 50722/60000 (84.54%)


  0%|          | 0/938 [00:00<?, ?it/s]
Epoch 1--Test set: Average loss: 0.0897, Validation Accuracy: 9791/10000 (97.91%)

epoch=2 loss=0.06962326169013977 batch_id=937: 100%|██████████| 938/938 [00:16<00:00, 58.42it/s]
Epoch 2--Train set: Average loss: 0.0696, Training Accuracy: 57869/60000 (96.45%)


  0%|          | 0/938 [00:00<?, ?it/s]
Epoch 2--Test set: Average loss: 0.0529, Validation Accuracy: 9848/10000 (98.48%)

epoch=3 loss=0.06028956174850464 batch_id=937: 100%|██████████| 938/938 [00:16<00:00, 58.40it/s]
Epoch 3--Train set: Average loss: 0.0603, Training Accuracy: 58436/60000 (97.39%)


  0%|          | 0/938 [00:00<?, ?it/s]
Epoch 3--Test set: Average loss: 0.0381, Validation Accuracy: 9890/10000 (98.90%)

epoch=4 loss=0.03670795261859894 batch_id=937: 100%|██████████| 938/938 [00:15<00:00, 59.96it/s]

Epoch 4--Train set: Average loss: 0.0367, Training Accuracy: 58675/60000 (97.79%)

  0%|          | 0/938 [00:00<?, ?it/s]
Epoch 4--Test set: Average loss: 0.0342, Validation Accuracy: 9896/10000 (98.96%)

epoch=5 loss=0.014482080936431885 batch_id=937: 100%|██████████| 938/938 [00:15<00:00, 58.90it/s]

Epoch 5--Train set: Average loss: 0.0145, Training Accuracy: 58736/60000 (97.89%)

  0%|          | 0/938 [00:00<?, ?it/s]
Epoch 5--Test set: Average loss: 0.0302, Validation Accuracy: 9906/10000 (99.06%)

epoch=6 loss=0.03699415922164917 batch_id=937: 100%|██████████| 938/938 [00:15<00:00, 59.43it/s]
Epoch 6--Train set: Average loss: 0.0370, Training Accuracy: 58877/60000 (98.13%)


  0%|          | 0/938 [00:00<?, ?it/s]
Epoch 6--Test set: Average loss: 0.0271, Validation Accuracy: 9919/10000 (99.19%)

epoch=7 loss=0.014394938945770264 batch_id=937: 100%|██████████| 938/938 [00:15<00:00, 59.38it/s]

Epoch 7--Train set: Average loss: 0.0144, Training Accuracy: 58961/60000 (98.27%)

  0%|          | 0/938 [00:00<?, ?it/s]
Epoch 7--Test set: Average loss: 0.0255, Validation Accuracy: 9919/10000 (99.19%)

epoch=8 loss=0.07803086936473846 batch_id=937: 100%|██████████| 938/938 [00:15<00:00, 59.59it/s]
Epoch 8--Train set: Average loss: 0.0780, Training Accuracy: 58978/60000 (98.30%)


  0%|          | 0/938 [00:00<?, ?it/s]
Epoch 8--Test set: Average loss: 0.0249, Validation Accuracy: 9916/10000 (99.16%)

epoch=9 loss=0.06453123688697815 batch_id=937: 100%|██████████| 938/938 [00:16<00:00, 57.78it/s]

Epoch 9--Train set: Average loss: 0.0645, Training Accuracy: 59031/60000 (98.39%)

  0%|          | 0/938 [00:00<?, ?it/s]
Epoch 9--Test set: Average loss: 0.0231, Validation Accuracy: 9933/10000 (99.33%)

epoch=10 loss=0.02337595820426941 batch_id=937: 100%|██████████| 938/938 [00:16<00:00, 57.73it/s]

Epoch 10--Train set: Average loss: 0.0234, Training Accuracy: 59111/60000 (98.52%)

  0%|          | 0/938 [00:00<?, ?it/s]
Epoch 10--Test set: Average loss: 0.0223, Validation Accuracy: 9927/10000 (99.27%)

epoch=11 loss=0.11296353489160538 batch_id=937: 100%|██████████| 938/938 [00:15<00:00, 59.74it/s]
Epoch 11--Train set: Average loss: 0.1130, Training Accuracy: 59104/60000 (98.51%)


  0%|          | 0/938 [00:00<?, ?it/s]
Epoch 11--Test set: Average loss: 0.0223, Validation Accuracy: 9928/10000 (99.28%)

epoch=12 loss=0.20057690143585205 batch_id=937: 100%|██████████| 938/938 [00:16<00:00, 58.14it/s]
Epoch 12--Train set: Average loss: 0.2006, Training Accuracy: 59122/60000 (98.54%)


  0%|          | 0/938 [00:00<?, ?it/s]
Epoch 12--Test set: Average loss: 0.0212, Validation Accuracy: 9937/10000 (99.37%)

epoch=13 loss=0.17706504464149475 batch_id=937: 100%|██████████| 938/938 [00:15<00:00, 59.62it/s]

Epoch 13--Train set: Average loss: 0.1771, Training Accuracy: 59165/60000 (98.61%)

  0%|          | 0/938 [00:00<?, ?it/s]
Epoch 13--Test set: Average loss: 0.0211, Validation Accuracy: 9935/10000 (99.35%)

epoch=14 loss=0.052569061517715454 batch_id=937: 100%|██████████| 938/938 [00:15<00:00, 59.92it/s]

Epoch 14--Train set: Average loss: 0.0526, Training Accuracy: 59231/60000 (98.72%)

  0%|          | 0/938 [00:00<?, ?it/s]
Epoch 14--Test set: Average loss: 0.0216, Validation Accuracy: 9928/10000 (99.28%)

epoch=15 loss=0.016775235533714294 batch_id=937: 100%|██████████| 938/938 [00:16<00:00, 58.14it/s]

Epoch 15--Train set: Average loss: 0.0168, Training Accuracy: 59196/60000 (98.66%)

  0%|          | 0/938 [00:00<?, ?it/s]
Epoch 15--Test set: Average loss: 0.0212, Validation Accuracy: 9934/10000 (99.34%)

epoch=16 loss=0.03582005202770233 batch_id=937: 100%|██████████| 938/938 [00:15<00:00, 59.22it/s]
Epoch 16--Train set: Average loss: 0.0358, Training Accuracy: 59266/60000 (98.78%)


  0%|          | 0/938 [00:00<?, ?it/s]
Epoch 16--Test set: Average loss: 0.0207, Validation Accuracy: 9932/10000 (99.32%)

epoch=17 loss=0.008993446826934814 batch_id=937: 100%|██████████| 938/938 [00:15<00:00, 58.98it/s]
Epoch 17--Train set: Average loss: 0.0090, Training Accuracy: 59241/60000 (98.73%)


  0%|          | 0/938 [00:00<?, ?it/s]
Epoch 17--Test set: Average loss: 0.0189, Validation Accuracy: 9935/10000 (99.35%)

epoch=18 loss=0.013608992099761963 batch_id=937: 100%|██████████| 938/938 [00:15<00:00, 59.91it/s]
Epoch 18--Train set: Average loss: 0.0136, Training Accuracy: 59305/60000 (98.84%)


  0%|          | 0/938 [00:00<?, ?it/s]
Epoch 18--Test set: Average loss: 0.0186, Validation Accuracy: 9941/10000 (99.41%)

epoch=19 loss=0.07923413068056107 batch_id=937: 100%|██████████| 938/938 [00:16<00:00, 58.62it/s]
Epoch 19--Train set: Average loss: 0.0792, Training Accuracy: 59272/60000 (98.79%)


  0%|          | 0/938 [00:00<?, ?it/s]
Epoch 19--Test set: Average loss: 0.0197, Validation Accuracy: 9938/10000 (99.38%)

epoch=20 loss=0.0036350637674331665 batch_id=937: 100%|██████████| 938/938 [00:15<00:00, 59.35it/s]
Epoch 20--Train set: Average loss: 0.0036, Training Accuracy: 59286/60000 (98.81%)



Epoch 20--Test set: Average loss: 0.0179, Validation Accuracy: 9942/10000 (99.42%)

#### Here is the step-by-step approach, being taken to achieve the required goals (meeting 99.4%+ accuracy, under the required parameters: <=20K and within 20 epochs) :
 
Fix up the required architecture/layer-organization...in terms of choosing the number of kernels for each layer, am trying to follow a 'multiple-of-8' numbers (from 8, 16, 24, 32...etc):

 Architecture will have 3 "components":
 
 1. One initial,"image-conv2d" layer at the begining, to convolve over the "original image" channels, am 
    initially providing 8 number of kernels for this 'separate' layer (which feeds in to the next  
    "2-conv2d-layer-block").It needs to be noted that this 1 initial layer + The 2 following layers(for
    "2-conv2d-layer-block") provide receptive field of 7x7 pixels(3->5->7) sufficient for the MNIST dataset's
    edges & gradient generation.In this evolution-experiment, kernel numbers started out as 8 initially, 
    but in the final architecture(which met the requirements) it became 16.
    
	
 2. conv2d-BLOCK with 2 layers (in this case):
    This block will be placed after the first "image-conv2d" layer, and one more instance of this block, will 
    also follow the transition-block (explained below) later.
    In this evolution-experiment, kernel numbers initially started out as (8-16) for the 'first-2-layer-block'
    & (8-16) for the 'second-2-layer-block', but in the final architecture(which met the requirements) it
    became (16-16) for the 'first-2-layer-block' & (24-24) for the 'second-2-layer-block'.
    
    
 3. Transition Blocks:
    1st transition layer, with both max-pool(k=2,s=2) and a-1x1-feature-merger kernel, following the
    'first-2-layer-block'.
    2nd transition layer, towards the end (following the 2nd conv2d-block) which does NOT have
    the maxpool (i.e. just has 1x1-feature-merger operator), and followed by the Global
    Average Pooling (GAP) layer leading upto the Softmax layer.    
    Here, at the end, we have another 'organization-possibility' i.e. we can also have a GAP layer followed
    by a 1x1 operator (which actually resembles a fully-connected(FC) layer in this case. (Note: for my 
    experiments, am finding that the 1x1, followed by GAP gave BETTER results, as compared to GAP followed
    by 1x1(in a FC-way).
    Hence, will be showing this evolution of incremental changes to 1x1->GAP organization rather than 
    GAP->1x1 (though, the first 2 networks(NW) below show both of them, but later iterations build upon
    the basic-NW having '1x1->GAP' organization only)


### Architecture (i.e. in terms of channels used across individual layers):

    
    i.   "image-conv2d" layer: o/p initially 8 channels (becomes 16 in the final one)
    
    ii.  2 similar conv2d blocks, with:
    
              1st layer: (8-16) o/p channels (becomes (16-16) in the final one)
			 
              2nd layer: (8-16) o/p channels (becomes (24-24) in the final one)
	      
    iii. 1x1 conv2d for 2nd transition-layer: 10 o/p channels(for num-classes=10 digits)
    
 
Input Channels/Image  |  Conv2d/Transform      | Output Channels | RF
---------------------|--------------|----------------------|----------------------
`28x28x1`              | `(3x3x1)x8`   |      `26x26x8`  |      `3x3`|      
` `              | `ReLU`   |      ` `  |      ` ` 
**26x26x8**             | **(3x3x8)x8**  |      **24x24x8** |      **5x5**
** **             | **ReLU**   |     ** **  |     ** **      
**24x24x8**             | **(3x3x8)x16**  |      **22x22x16** |      **7x7**  
** **             | **ReLU**   |     ** **  |     ** **                       
*22x22x16*             |   *MP(2x2)*    |      *11x11x16*   |      *8x8*                      
*11x11x16*             | *(1x1x16)x8*  |      *11x11x8*    |      *8x8* 
** **             | *ReLU*   |     * *   |     * *
**11x11x8**             | **(3x3x8)x8**  |      **9x9x8** |      **12x12** 
** **             | **ReLU**   |     ** **  |     ** **   
**9x9x8**               | **(3x3x8)x16**  |      **7x7x16**  |      **16x16** 
** **             | **ReLU**   |     ** **  |     ** **    
*7x7x16*               | *(1x1x16)x10*  |      *7x7x10*    |      *16x16*  (NO RELU at the o/p of this layer)    
7x7x10               | GAP  LAYER   |      1x10          |


    iv. The 2nd variant could've been, as below, where the last 1x1 is actually behaving like a FC-layer:

Input Channels/Image  |  Conv2d/Transform      | Output Channels | RF
---------------------|--------------|----------------------|----------------------
`28x28x1`              | `(3x3x1)x8`   |      `26x26x8`  |      `3x3`
` `              | `ReLU`   |      ` `  |      ` `  
**26x26x8**             | **(3x3x8)x8**  |      **24x24x8** |      **5x5**  
** **             | **ReLU**   |     ** **  |     ** **      
**24x24x8**             | **(3x3x8)x16**  |      **22x22x16** |      **7x7**  
** **             | **ReLU**   |     ** **  |     ** **      
*22x22x16*             |   *MP(2x2)*    |      *11x11x16*   |      *8x8*                      
*11x11x16*             | *(1x1x16)x8*  |      *11x11x8*    |      *8x8* 
 ** **             | *ReLU*   |     * *   |     * *      
**11x11x8**             | **(3x3x8)x8**  |      **9x9x8** |      **12x12**
** **             | **ReLU**   |     ** **  |     ** **   
**9x9x8**               | **(3x3x8)x16**  |      **7x7x16**  |      **16x16** (NO RELU at the o/p of this layer)            
7x7x16               | GAP  LAYER   |      1x16          |  (the output, though can be written as 1x1x16, but is 1-D, i.e.1x16)
*1x1x16*               | *(1x1x16)x10*  |      *1x10*    | (behaves as fully-connected layer for the 1-D data from GAP)


    v. As mentioned earlier, have found better results for the 1x1->GAP option, rather than GAP->1x1(or, FC)
       hence the increments of Batch Normalization, dropout etc are made with this arrangement.
       
    vi. At the end, following Architecture (same as the first-table above, but with increased number of channels,
        like below is found to achieve the required goal: 14,112 params, >99.4% accuracy, in less than 20 epochs 
        (while 'sticking-to' the same learning rate as given in the original code, as Learning-rate tuning is not 
        to be experimented in this session)
        

Input Channels/Image  |  Conv2d/Transform      | Output Channels | RF
---------------------|--------------|----------------------|----------------------
`28x28x1`              | `(3x3x1)x16`   |      `26x26x16`  |      `3x3`
` `              | `BN(16)`   |      ` `  |      ` `
` `              | `Dropout(3%)`   |      ` `  |      ` `
` `              | `ReLU`   |      ` `  |      ` `  
**26x26x16**             | **(3x3x16)x16**  |      **24x24x16** |      **5x5** 
** **             | **BN(16)**   |     ** **  |     ** **
** **             | **Dropout(3%)**   |     ** **  |     ** **
** **             | **ReLU**   |     ** **  |     ** **              
**24x24x16**             | **(3x3x16)x16**  |      **22x22x16** |      **7x7** 
** **             | **BN(16)**   |     ** **  |     ** **
** **             | **Dropout(3%)**   |     ** **  |     ** **
** **             | **ReLU**   |     ** **  |     ** **              
*22x22x16*             |   *MP(2x2)*    |      *11x11x16*   |      *8x8*                      
*11x11x16*             | *(1x1x16)x16*  |      *11x11x16*    |      *8x8*   (Here  a conversion from 16 input channels to -> 16 output channels, using 1x1 might look like an 'oddity', but as 1x1 is a feature-merging operator, which works along-with BackProp, to get "contextually-linked" channels as outputs for the upstream layers (from the 'scattered' lower level features)
** **            | *BN(16)*   |     * *   |     * * 
** **             | *Dropout(3%)*   |     * *   |     * * 
** **             | *ReLU*   |     * *   |     * *                         
**11x11x16**             | **(3x3x16)x24**  |      **9x9x24** |      **12x12**  
** **             | **BN(24)**   |     ** **  |     ** **
** **             | **Dropout(3%)**   |     ** **  |     ** **
** **             | **ReLU**   |     ** **  |     ** **                         
**9x9x24**               | **(3x3x24)x24**  |      **7x7x24**  |      **16x16**   
** **             | **BN(24)**   |     ** **  |     ** **
** **             | **Dropout(3%)**   |     ** **  |     ** **
** **             | **ReLU**   |     ** **  |     ** **                          
*7x7x24*               | *(1x1x24)x10*  |      *7x7x10*    |      *16x16*   (NO RELU at the o/p of this layer)   
7x7x10               | GAP  LAYER   |      1x10          |     


#### First NW:

Input Channels/Image  |  Conv2d/Transform      | Output Channels | RF
---------------------|--------------|----------------------|----------------------
`28x28x1`              | `(3x3x1)x8`   |      `26x26x8`  |      `3x3`|      
` `              | `ReLU`   |      ` `  |      ` ` 
**26x26x8**             | **(3x3x8)x8**  |      **24x24x8** |      **5x5**
** **             | **ReLU**   |     ** **  |     ** **      
**24x24x8**             | **(3x3x8)x16**  |      **22x22x16** |      **7x7**  
** **             | **ReLU**   |     ** **  |     ** **                       
*22x22x16*             |   *MP(2x2)*    |      *11x11x16*   |      *8x8*                      
*11x11x16*             | *(1x1x16)x8*  |      *11x11x8*    |      *8x8* 
** **             | *ReLU*   |     * *   |     * *
**11x11x8**             | **(3x3x8)x8**  |      **9x9x8** |      **12x12** 
** **             | **ReLU**   |     ** **  |     ** **   
**9x9x8**               | **(3x3x8)x16**  |      **7x7x16**  |      **16x16** 
** **             | **ReLU**   |     ** **  |     ** **    
*7x7x16*               | *(1x1x16)x10*  |      *7x7x10*    |      *16x16*  (NO RELU at the o/p of this layer)    
7x7x10               | GAP  LAYER   |      1x10          |

	- First observation: the parameters are 3,816 the max validation accuracy reached: ~98.25%
	- The logs and accuracy plot above show that there is some overfitting in terms of in terms of 
	  the accuracy plot, both the train and test accuracies seem to be running in close step.With not
	  much "potential" for the test/validation accuracy to increase, with a corresponding increase
	  in training accuracy. 
	- Additionally, with required 20 epochs, the network capacity seems to be not sufficient to meet
	  the goal. 
	- But, despite an urgent need for increased channels, we would still like to see, if with added
	  Batch-Norm, Dropout, what effects could be seen?  
	- Meantime, we will explore the 2nd variant below as well to see where it stands.

#### 2nd NW: 
this is just the GAP->1x1(FC) variant...this variant is not being taken forward for incremental changes (as it was found to be lagging behind as compared to the 1x1->GAP variant discussed earlier)

Input Channels/Image  |  Conv2d/Transform      | Output Channels | RF
---------------------|--------------|----------------------|----------------------
`28x28x1`              | `(3x3x1)x8`   |      `26x26x8`  |      `3x3`
` `              | `ReLU`   |      ` `  |      ` `  
**26x26x8**             | **(3x3x8)x8**  |      **24x24x8** |      **5x5**  
** **             | **ReLU**   |     ** **  |     ** **      
**24x24x8**             | **(3x3x8)x16**  |      **22x22x16** |      **7x7**  
** **             | **ReLU**   |     ** **  |     ** **      
*22x22x16*             |   *MP(2x2)*    |      *11x11x16*   |      *8x8*                      
*11x11x16*             | *(1x1x16)x8*  |      *11x11x8*    |      *8x8* 
 ** **             | *ReLU*   |     * *   |     * *      
**11x11x8**             | **(3x3x8)x8**  |      **9x9x8** |      **12x12**
** **             | **ReLU**   |     ** **  |     ** **   
**9x9x8**               | **(3x3x8)x16**  |      **7x7x16**  |      **16x16** (NO RELU at the o/p of this layer)            
7x7x16               | GAP  LAYER   |      1x16          |  (the output, though can be written as 1x1x16, but is 1-D, i.e.1x16)
*1x1x16*               | *(1x1x16)x10*  |      *1x10*    | (behaves as fully-connected layer for the 1-D data from GAP)

	- First observation:  the parameters are 3,816(same as the 1st variant) max acc reached--98.26%
	- The other results for this seem to be same at this point, but later on it has been found
	  during the incremental evolution with BN, Dropout etc, the results with this variant weren't
	  comparable to the first (and hence the 1st one itself, has been taken thru the next set of changes)

#### 3rd NW: 

added Batch Normalization at each layer(we should expect some increase in accuracy in this case)

Input Channels/Image  |  Conv2d/Transform      | Output Channels | RF
---------------------|--------------|----------------------|----------------------
`28x28x1`              | `(3x3x1)x8`   |      `26x26x8`  |      `3x3`|  
` `              | `BN(8)`   |      ` `  |      ` `
` `              | `ReLU`   |      ` `  |      ` ` 
**26x26x8**             | **(3x3x8)x8**  |      **24x24x8** |      **5x5**
** **             | **BN(8)**   |     ** **  |     ** **
** **             | **ReLU**   |     ** **  |     ** **      
**24x24x8**             | **(3x3x8)x16**  |      **22x22x16** |      **7x7** 
** **             | **BN(16)**   |     ** **  |     ** **
** **             | **ReLU**   |     ** **  |     ** **                       
*22x22x16*             |   *MP(2x2)*    |      *11x11x16*   |      *8x8*                      
*11x11x16*             | *(1x1x16)x8*  |      *11x11x8*    |      *8x8* 
** **            | *BN(8)*   |     * *   |     * * 
** **             | *ReLU*   |     * *   |     * *
**11x11x8**             | **(3x3x8)x8**  |      **9x9x8** |      **12x12** 
** **             | **BN(8)**   |     ** **  |     ** **
** **             | **ReLU**   |     ** **  |     ** **   
**9x9x8**               | **(3x3x8)x16**  |      **7x7x16**  |      **16x16** 
** **             | **BN(16)**   |     ** **  |     ** **
** **             | **ReLU**   |     ** **  |     ** **    
*7x7x16*               | *(1x1x16)x10*  |      *7x7x10*    |      *16x16*  (NO RELU at the o/p of this layer)    
7x7x10               | GAP  LAYER   |      1x10          |

	- First observation:  Total params: 3,944 (which is expectedly, higher as compared to the 1st NW) 
	  & the max validation accuracy reaches: ~98.94%
	- we got little better in terms of the reached accuracy but expectedly, far from the required accuracy
	  goal.
	- Also in terms of the accuracy plot & logs, we could see that the further 'potential-to-increase',
	  for the validation accuracy, has not yet opened up (it is overfitting...i.e. the training accuracy
	  has reached 99.08% but the validation acuracy still under ~98.8%, hence not much scope for further
	  increase with a corresponding increase in training accuracy )
	- just like the earlier NWs, this one can't meet the goal within the required 20 epochs(capacity 
	  boost required)

#### 4th NW:

added Dropout as well at each layer apart from the BN(we should expect lesser overfitting i.e. an increase in the 
"potential" to increase validation accuracy with corresponding increase in training-accuracy) i.e. a gap opens up. 
The value for 3% for Dropout was giving a better results as compared to other tested-ranges from 1-10%.

Input Channels/Image  |  Conv2d/Transform      | Output Channels | RF
---------------------|--------------|----------------------|----------------------
`28x28x1`              | `(3x3x1)x8`   |      `26x26x8`  |      `3x3`|  
` `              | `BN(8)`   |      ` `  |      ` `
` `              | `Dropout(3%)`   |      ` `  |      ` `
` `              | `ReLU`   |      ` `  |      ` ` 
**26x26x8**             | **(3x3x8)x8**  |      **24x24x8** |      **5x5**
** **             | **BN(8)**   |     ** **  |     ** **
** **             | **Dropout(3%)**   |     ** **  |     ** **
** **             | **ReLU**   |     ** **  |     ** **      
**24x24x8**             | **(3x3x8)x16**  |      **22x22x16** |      **7x7** 
** **             | **BN(16)**   |     ** **  |     ** **
** **             | **Dropout(3%)**   |     ** **  |     ** **
** **             | **ReLU**   |     ** **  |     ** **                       
*22x22x16*             |   *MP(2x2)*    |      *11x11x16*   |      *8x8*                      
*11x11x16*             | *(1x1x16)x8*  |      *11x11x8*    |      *8x8* 
** **            | *BN(8)*   |     * *   |     * * 
** **             | *Dropout(3%)*   |     * *   |     * * 
** **             | *ReLU*   |     * *   |     * *
**11x11x8**             | **(3x3x8)x8**  |      **9x9x8** |      **12x12** 
** **             | **BN(8)**   |     ** **  |     ** **
** **             | **Dropout(3%)**   |     ** **  |     ** **
** **             | **ReLU**   |     ** **  |     ** **   
**9x9x8**               | **(3x3x8)x16**  |      **7x7x16**  |      **16x16** 
** **             | **BN(16)**   |     ** **  |     ** **
** **             | **Dropout(3%)**   |     ** **  |     ** **
** **             | **ReLU**   |     ** **  |     ** **    
*7x7x16*               | *(1x1x16)x10*  |      *7x7x10*    |      *16x16*  (NO RELU at the o/p of this layer)    
7x7x10               | GAP  LAYER   |      1x10          |

	- First observation:  Total params: 3,944 (which is expectedly,same as compared to the last NW as
	  dropout doesn't add params) & the max validation accuracy reaches: ~98.4% (though this seems almost
	  comparable to the achieved max in last iteration, but in case of adding Dropout, we have overcome the
	  overfitting issue, by opening up the gap between training and test accuracies...i.e. now we can see 
	  that the test accuracy value like ~98% is achieved while the training accuracy is still at ~96%, which
	  has opened up the potential for further increase in the validation accuracy, given we have a required 
	  number of epochs and NW-capacity with us.
	- Also in terms of the accuracy plot, both the train and test accuracies seem to be maintaining almost 
	  consistent gap (training accuracy growth is looking stagnant though, but some scope is open)
	- As for this architecture-option we are required to stick-to the given Learning rate value, hence 
	  that aspect won't be explored further.
	- But to achieve the required goal we will have to go for a capacity increase now, the next iteration
	  tries to do the same.

#### 5th NW:

while retaining the same template for the Batch Normalization, Dropout and layering, we now increase the channels
to follow the layering as given below.The value for 3% for Dropout was giving a better results as compared to other
test-ranges from 1-10%. "|16|--|16-16|--|Transition|--|24-24|--|Transition|--|GAP|--|softmax-classifier|" as in the 
table below

Input Channels/Image  |  Conv2d/Transform      | Output Channels | RF
---------------------|--------------|----------------------|----------------------
`28x28x1`              | `(3x3x1)x16`   |      `26x26x16`  |      `3x3`
` `              | `BN(16)`   |      ` `  |      ` `
` `              | `Dropout(3%)`   |      ` `  |      ` `
` `              | `ReLU`   |      ` `  |      ` `  
**26x26x16**             | **(3x3x16)x16**  |      **24x24x16** |      **5x5** 
** **             | **BN(16)**   |     ** **  |     ** **
** **             | **Dropout(3%)**   |     ** **  |     ** **
** **             | **ReLU**   |     ** **  |     ** **              
**24x24x16**             | **(3x3x16)x16**  |      **22x22x16** |      **7x7** 
** **             | **BN(16)**   |     ** **  |     ** **
** **             | **Dropout(3%)**   |     ** **  |     ** **
** **             | **ReLU**   |     ** **  |     ** **              
*22x22x16*             |   *MP(2x2)*    |      *11x11x16*   |      *8x8*                      
*11x11x16*             | *(1x1x16)x16*  |      *11x11x16*    |      *8x8* (Here  a conversion from 16 input channels to -> 16 output channels, using 1x1 might look like an 'oddity', but as 1x1 is a feature-merging operator, which works along-with BackProp, to get "contextually-linked" channels as outputs for the upstream layers (from the 'scattered' lower level features)  
** **            | *BN(16)*   |     * *   |     * * 
** **             | *Dropout(3%)*   |     * *   |     * * 
** **             | *ReLU*   |     * *   |     * *                         
**11x11x16**             | **(3x3x16)x24**  |      **9x9x24** |      **12x12**  
** **             | **BN(24)**   |     ** **  |     ** **
** **             | **Dropout(3%)**   |     ** **  |     ** **
** **             | **ReLU**   |     ** **  |     ** **                         
**9x9x24**               | **(3x3x24)x24**  |      **7x7x24**  |      **16x16**   
** **             | **BN(24)**   |     ** **  |     ** **
** **             | **Dropout(3%)**   |     ** **  |     ** **
** **             | **ReLU**   |     ** **  |     ** **                          
*7x7x24*               | *(1x1x24)x10*  |      *7x7x10*    |      *16x16*   (NO RELU at the o/p of this layer)   
7x7x10               | GAP  LAYER   |      1x10          |     

	- First observation:  Total params: 14,112 (which is expected,given higher NW capacity) & the max 
	  validation accuracy reaches: ~99.42%
	- This case meets the required goal of getting an accuracy of >= 99.4% (appeared 2 times during
	  training-epochs), the parameters: 14,112 < 20K (requirement) and came under 20 epochs.
